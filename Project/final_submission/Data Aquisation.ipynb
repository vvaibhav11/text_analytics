{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60cbdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5cea6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirlineQualityScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.download_page()\n",
    "        self.info_list = ['Type Of Traveller', 'Seat Type','Route','Date Flown']\n",
    "        self.ratings = ['Seat Comfort', 'Cabin Staff Service', 'Food & Beverages', 'Inflight Entertainment', 'Ground Service', 'Wifi & Connectivity', 'Value For Money']\n",
    "\n",
    "    def download_page(self):\n",
    "        # method for downloading the webpage\n",
    "        self.page = requests.get(self.url).text\n",
    "    \n",
    "    def rating_info_search(self, html_table, topic, content_type='td'):\n",
    "        locate = html_table.find('td', text=topic)\n",
    "        if locate is not None:\n",
    "            value = locate.findNext(content_type).text\n",
    "        else:\n",
    "            value = None\n",
    "        return value\n",
    "    \n",
    "    def count_star(self, html_table, topic):\n",
    "        star = html_table.find('td', text=topic)\n",
    "        count = 0\n",
    "        if star is not None:\n",
    "            for i in range(5):\n",
    "                star = star.findNext('span')\n",
    "                if \"star fill\" in str(star):\n",
    "                    count += 1\n",
    "        return count\n",
    "\n",
    "    def scrape_data(self):\n",
    "        soup = BeautifulSoup(self.page, \"html.parser\")\n",
    "        data = {'title':[],'user':[],'user_type':[],'review':[],'publish_date':[]}\n",
    "        \n",
    "        reviews = soup.find_all(\"div\",{\"class\": \"body\"})\n",
    "        for review in reviews:\n",
    "            title = review.find(class_=\"text_header\").text\n",
    "            data['title'].append(title.replace('\"',''))\n",
    "            user_name = review.find(\"span\",{\"itemprop\":\"name\"}).text\n",
    "            data['user'].append(user_name)\n",
    "            publish_date = review.find(\"time\",{'itemprop':\"datePublished\"}).text\n",
    "            data['publish_date'].append(publish_date)\n",
    "            \n",
    "            review_raw = review.find('div',{'itemprop':\"reviewBody\"}).text\n",
    "            if '|' in review_raw:\n",
    "                review_split = review_raw.split('|')\n",
    "                data['user_type'].append(''.join(ch for ch in review_split[0] if ch.isalnum()))\n",
    "                data['review'].append(review_split[1])\n",
    "            else:\n",
    "                data['user_type'].append('Not Verified')\n",
    "                data['review'].append(review_raw)\n",
    "            \n",
    "            review_table = review.find('table', {'class': 'review-ratings'})\n",
    "            for topic in self.info_list:\n",
    "                topic_value = self.rating_info_search(review_table,topic)\n",
    "                if topic not in data.keys():\n",
    "                    data[topic] = [topic_value]\n",
    "                else:\n",
    "                    data[topic].append(topic_value)\n",
    "            for rating_item in self.ratings:\n",
    "                rating = self.count_star(review_table,rating_item)\n",
    "                if rating_item not in data.keys():\n",
    "                    data[rating_item] = [rating]\n",
    "                else:\n",
    "                    data[rating_item].append(rating)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "#for url in urls:\n",
    "\n",
    "#    x = TripHotelScraper(url)\n",
    "\n",
    "#    print(x.scrape_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cfbee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aircanada_review = \"https://www.airlinequality.com/airline-reviews/air-canada\"\n",
    "#x = AirlineQualityScraper(aircanada_review)\n",
    "#scrapped = x.scrape_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5ce44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pages(url,n_pages):\n",
    "    print('Scrapping page:', 1)\n",
    "    page_1 = AirlineQualityScraper(url)\n",
    "    scrapped = page_1.scrape_data()\n",
    "    scrapped['page'] = [1]*len(scrapped['user'])\n",
    "    for i in range(2,n_pages+1): \n",
    "        print('Scrapping page:', i)\n",
    "        page = AirlineQualityScraper(url+'/page/'+str(i)+'/')\n",
    "        data_scrapped = page.scrape_data()\n",
    "        data_scrapped['page'] = [i]*len(data_scrapped['user'])\n",
    "        for k, v in data_scrapped.items():\n",
    "            scrapped[k] += data_scrapped[k]\n",
    "    return scrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfc6d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping page: 1\n",
      "Scrapping page: 2\n",
      "Scrapping page: 3\n",
      "Scrapping page: 4\n",
      "Scrapping page: 5\n",
      "Scrapping page: 6\n",
      "Scrapping page: 7\n",
      "Scrapping page: 8\n",
      "Scrapping page: 9\n",
      "Scrapping page: 10\n",
      "Scrapping page: 11\n",
      "Scrapping page: 12\n",
      "Scrapping page: 13\n",
      "Scrapping page: 14\n",
      "Scrapping page: 15\n",
      "Scrapping page: 16\n",
      "Scrapping page: 17\n",
      "Scrapping page: 18\n",
      "Scrapping page: 19\n",
      "Scrapping page: 20\n",
      "Scrapping page: 21\n",
      "Scrapping page: 22\n",
      "Scrapping page: 23\n",
      "Scrapping page: 24\n",
      "Scrapping page: 25\n",
      "Scrapping page: 26\n",
      "Scrapping page: 27\n",
      "Scrapping page: 28\n",
      "Scrapping page: 29\n",
      "Scrapping page: 30\n",
      "Scrapping page: 31\n",
      "Scrapping page: 32\n",
      "Scrapping page: 33\n",
      "Scrapping page: 34\n",
      "Scrapping page: 35\n",
      "Scrapping page: 36\n",
      "Scrapping page: 37\n",
      "Scrapping page: 38\n",
      "Scrapping page: 39\n",
      "Scrapping page: 40\n",
      "Scrapping page: 41\n",
      "Scrapping page: 42\n",
      "Scrapping page: 43\n",
      "Scrapping page: 44\n",
      "Scrapping page: 45\n",
      "Scrapping page: 46\n",
      "Scrapping page: 47\n",
      "Scrapping page: 48\n",
      "Scrapping page: 49\n",
      "Scrapping page: 50\n",
      "Scrapping page: 51\n",
      "Scrapping page: 52\n",
      "Scrapping page: 53\n",
      "Scrapping page: 54\n",
      "Scrapping page: 55\n",
      "Scrapping page: 56\n",
      "Scrapping page: 57\n",
      "Scrapping page: 58\n",
      "Scrapping page: 59\n",
      "Scrapping page: 60\n",
      "Scrapping page: 61\n",
      "Scrapping page: 62\n",
      "Scrapping page: 63\n",
      "Scrapping page: 64\n",
      "Scrapping page: 65\n",
      "Scrapping page: 66\n",
      "Scrapping page: 67\n",
      "Scrapping page: 68\n",
      "Scrapping page: 69\n",
      "Scrapping page: 70\n",
      "Scrapping page: 71\n",
      "Scrapping page: 72\n",
      "Scrapping page: 73\n",
      "Scrapping page: 74\n",
      "Scrapping page: 75\n",
      "Scrapping page: 76\n",
      "Scrapping page: 77\n",
      "Scrapping page: 78\n",
      "Scrapping page: 79\n",
      "Scrapping page: 80\n",
      "Scrapping page: 81\n",
      "Scrapping page: 82\n",
      "Scrapping page: 83\n",
      "Scrapping page: 84\n",
      "Scrapping page: 85\n",
      "Scrapping page: 86\n",
      "Scrapping page: 87\n",
      "Scrapping page: 88\n",
      "Scrapping page: 89\n",
      "Scrapping page: 90\n",
      "Scrapping page: 91\n",
      "Scrapping page: 92\n",
      "Scrapping page: 93\n",
      "Scrapping page: 94\n",
      "Scrapping page: 95\n",
      "Scrapping page: 96\n",
      "Scrapping page: 97\n",
      "Scrapping page: 98\n",
      "Scrapping page: 99\n",
      "Scrapping page: 100\n",
      "Scrapping page: 101\n",
      "Scrapping page: 102\n",
      "Scrapping page: 103\n",
      "Scrapping page: 104\n",
      "Scrapping page: 105\n",
      "Scrapping page: 106\n",
      "Scrapping page: 107\n",
      "Scrapping page: 108\n",
      "Scrapping page: 109\n",
      "Scrapping page: 110\n",
      "Scrapping page: 111\n",
      "Scrapping page: 112\n",
      "Scrapping page: 113\n",
      "Scrapping page: 114\n",
      "Scrapping page: 115\n",
      "Scrapping page: 116\n",
      "Scrapping page: 117\n",
      "Scrapping page: 118\n",
      "Scrapping page: 119\n",
      "Scrapping page: 120\n",
      "Scrapping page: 121\n",
      "Scrapping page: 122\n",
      "Scrapping page: 123\n",
      "Scrapping page: 124\n",
      "Scrapping page: 125\n",
      "Scrapping page: 126\n",
      "Scrapping page: 127\n",
      "Scrapping page: 128\n",
      "Scrapping page: 129\n",
      "Scrapping page: 130\n",
      "Scrapping page: 131\n",
      "Scrapping page: 132\n",
      "Scrapping page: 133\n",
      "Scrapping page: 134\n",
      "Scrapping page: 135\n",
      "Scrapping page: 136\n",
      "Scrapping page: 137\n",
      "Scrapping page: 138\n",
      "Scrapping page: 139\n",
      "Scrapping page: 140\n",
      "Scrapping page: 141\n",
      "Scrapping page: 142\n",
      "Scrapping page: 143\n",
      "Scrapping page: 144\n",
      "Scrapping page: 145\n",
      "Scrapping page: 146\n",
      "Scrapping page: 147\n",
      "Scrapping page: 148\n",
      "Scrapping page: 149\n",
      "Scrapping page: 150\n",
      "Scrapping page: 151\n",
      "Scrapping page: 152\n",
      "Scrapping page: 153\n",
      "Scrapping page: 154\n",
      "Scrapping page: 155\n",
      "Scrapping page: 156\n",
      "Scrapping page: 157\n",
      "Scrapping page: 158\n",
      "Scrapping page: 159\n",
      "Scrapping page: 160\n",
      "Scrapping page: 161\n",
      "Scrapping page: 162\n",
      "Scrapping page: 163\n",
      "Scrapping page: 164\n",
      "Scrapping page: 165\n"
     ]
    }
   ],
   "source": [
    "data_air_canada = scrape_pages(aircanada_review,165)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efa11091",
   "metadata": {},
   "outputs": [],
   "source": [
    "aircanada_review = pd.DataFrame(data_air_canada)\n",
    "aircanada_review.to_csv('AirCanada_review_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fe3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = stopwords.words('english')\n",
    "lst_of_tokenized = comments['comment_tokenized']\n",
    "for sentence in lst_of_tokenized:\n",
    "    new_sentence = []\n",
    "    for s in sentence:\n",
    "        if s in string.punctuation or s == '..':\n",
    "            sentence.remove(s)\n",
    "        if s in stop_words:\n",
    "            sentence.remove(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
